import numpy as np
import matplotlib.pyplot as plt

# Configuración del Laberinto
laberinto_dim = 100

# Crear el laberinto aleatorio
np.random.seed(42)  # Semilla para reproducibilidad
laberinto = np.random.choice([0, 1], size=(laberinto_dim, laberinto_dim), p=[0.8, 0.2])

# Establecer el inicio (0,0) y la meta (99,99)
laberinto[0, 0] = 0  # Punto inicial
laberinto[99, 99] = 2  # Meta

# Visualización inicial del laberinto
plt.figure(figsize=(8, 8))
plt.imshow(laberinto, cmap='viridis')
plt.title("Laberinto Inicial")
plt.show()

# Función para validar movimientos
def validar_movimiento(laberinto, x, y):
    if x < 0 or x >= laberinto_dim or y < 0 or y >= laberinto_dim:
        return False
    return laberinto[x, y] != 1

# Función para mover al agente
def mover_agente(laberinto, x, y, accion):
    movimientos = {
        0: (-1, 0),  # Arriba
        1: (1, 0),   # Abajo
        2: (0, -1),  # Izquierda
        3: (0, 1)    # Derecha
    }
    dx, dy = movimientos[accion]
    nx, ny = x + dx, y + dy
    if validar_movimiento(laberinto, nx, ny):
        return nx, ny
    return x, y

# Parámetros del algoritmo Q-Learning
alpha = 0.1  # Tasa de aprendizaje
gamma = 0.9  # Factor de descuento
epsilon = 0.2  # Probabilidad de exploración
episodios = 1000

# Inicializar la tabla Q
q_table = np.zeros((laberinto_dim, laberinto_dim, 4))

# Entrenamiento del agente
for episodio in range(episodios):
    x, y = 0, 0  # Punto de inicio
    while laberinto[x, y] != 2:  # Mientras no alcance la meta
        # Selección de acción: exploración vs explotación
        if np.random.rand() < epsilon:
            accion = np.random.choice(4)  # Acción aleatoria
        else:
            accion = np.argmax(q_table[x, y])  # Mejor acción conocida

        # Mover agente y obtener nueva posición
        nx, ny = mover_agente(laberinto, x, y, accion)

        # Asignar recompensa
        recompensa = 1 if laberinto[nx, ny] == 2 else -0.1

        # Actualizar tabla Q
        q_table[x, y, accion] = (1 - alpha) * q_table[x, y, accion] + \
                                alpha * (recompensa + gamma * np.max(q_table[nx, ny]))

        # Actualizar posición del agente
        x, y = nx, ny

# Evaluación del modelo: Prueba del agente
x, y = 0, 0
recorrido = [(x, y)]

while laberinto[x, y] != 2:  # Mientras no alcance la meta
    accion = np.argmax(q_table[x, y])  # Elegir la mejor acción
    x, y = mover_agente(laberinto, x, y, accion)
    recorrido.append((x, y))

# Visualización del recorrido final
plt.figure(figsize=(8, 8))
plt.imshow(laberinto, cmap='viridis')
for (i, j) in recorrido:
    plt.plot(j, i, 'ro')  # Marcar el recorrido del agente
plt.title("Recorrido del Agente")
plt.show()
